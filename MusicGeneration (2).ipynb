{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXD6gvGbcJtn"
      },
      "outputs": [],
      "source": [
        "!pip install comet_ml > /dev/null 2>&1\n",
        "import comet_ml\n",
        "# TODO: ENTER YOUR API KEY HERE!! instructions above\n",
        "COMET_API_KEY = \"5i0jFaHpBypwOFgN3tQlxtMoJ\"\n",
        "\n",
        "# Import PyTorch and other relevant libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Download and import the MIT Introduction to Deep Learning package\n",
        "!pip install mitdeeplearning --quiet\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "# Import all remaining packages\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "from scipy.io.wavfile import write\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1\n",
        "\n",
        "\n",
        "# Check that we are using a GPU, if not switch runtimes\n",
        "#   using Runtime > Change Runtime Type > GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "assert COMET_API_KEY != \"\", \"Please insert your Comet API Key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n22lvNAecw02"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "# Print one of the songs to inspect it in greater detail!\n",
        "example_song = songs[0]\n",
        "print(\"\\nExample song: \")\n",
        "print(example_song)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js0DOk3Pde8B"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "# let's try on the songs\n",
        "Example_song = songs[0]\n",
        "\n",
        "print(\"Example song\\n\", Example_song)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily convert a song in ABC notation to an audio waveform and play it back. Be patient for this conversion to run, it can take some time."
      ],
      "metadata": {
        "id": "WznuyI2D5WU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OFV-OOaeHG8"
      },
      "outputs": [],
      "source": [
        "# Convert the ABC notation to audio file and ilsten to it\n",
        "mdl.lab1.play_song(Example_song)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important thing to think about is that this notation of music does not simply contain information on the notes being played, but additionally there is meta information such as the song title, key, and tempo. How does the number of different characters that are present in the text file impact the complexity of the learning problem? This will become important soon, when we generate a numerical representation for the text data."
      ],
      "metadata": {
        "id": "y9EZcnrb791u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB_OURClwKh5"
      },
      "outputs": [],
      "source": [
        "# Join our list of song strings into a single string containing all songs\n",
        "songs_joined = '\\n\\n'.join(songs)\n",
        "\n",
        "# Find all unique characters in the joined string\n",
        "vocab = sorted(set(songs_joined))\n",
        "print(f\"There is {len(vocab)} unique characters in the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Process the dataset for the learning task"
      ],
      "metadata": {
        "id": "-2RItgfD8jTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a step back and consider our prediction task. We're trying to train an RNN model to learn patterns in ABC music, and then use this model to generate (i.e., predict) a new piece of music based on this learned information.\n",
        "\n",
        "Breaking this down, what we're really asking the model is: given a character, or a sequence of characters, what is the most probable next character? We'll train the model to perform this task.\n",
        "\n",
        "To achieve this, we will input a sequence of characters to the model, and train the model to predict the output, that is, the following character at each time step. RNNs maintain an internal state that depends on previously seen elements, so information about all characters seen up until a given moment will be taken into account in generating the prediction.\n",
        "\n",
        "\n",
        "## Vectorize the text\n",
        "\n",
        "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters. Recall that we just identified the unique characters present in the text."
      ],
      "metadata": {
        "id": "aeaMD58N9N2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Define numerical representation of text ###\n",
        "\n",
        "# Create a mapping from character to unique index.\n",
        "# For example, to get the index of the character \"d\",\n",
        "#  we can evaluate `char2idx[\"d\"]`.\n",
        "\n",
        "char2idx = {u : i for i, u in enumerate(vocab)}\n",
        "\n",
        "# Create a mapping from indices to characters. This is\n",
        "#  the inverse of char2idx and allows us to convert back\n",
        "#  from unique index to the character in our vocabulary.\n",
        "\n",
        "idx2char = np.array(vocab)\n"
      ],
      "metadata": {
        "id": "7lM7tBks9O6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us an integer representation for each character. Observe that the unique characters (i.e., our vocabulary) in the text are mapped as indices from 0 to len(unique). Let's take a peek at this numerical representation of our dataset:"
      ],
      "metadata": {
        "id": "vji_0G1h-4pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('{')\n",
        "for i, char in enumerate(char2idx):\n",
        "    if i == 20:\n",
        "        break\n",
        "    print(f'  {repr(char):4s}: {char2idx[char]:3d},')\n",
        "print('  ...\\n}')\n"
      ],
      "metadata": {
        "id": "zdO29PiT-o2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Vectorize the songs string ###\n",
        "\n",
        "def vectorize_string(string):\n",
        "  return np.array([char2idx[c] for c in string], dtype=np.int32)\n",
        "\n",
        "vectorized_songs = vectorize_string(songs_joined)"
      ],
      "metadata": {
        "id": "87RS2SXZ_Hwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at how the first part of the text is mapped to an integer representation:"
      ],
      "metadata": {
        "id": "tfWkyFNzCb4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
        "\n",
        "# check that vectorized_songs is a numpy array\n",
        "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
      ],
      "metadata": {
        "id": "dfVV8xtGCcYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create training examples and targets\n",
        "Our next step is to actually divide the text into example sequences that we'll use during training. Each input sequence that we feed into our RNN will contain seq_length characters from the text. We'll also need to define a target sequence for each input sequence, which will be used in training the RNN to predict the next character. For each input, the corresponding target will contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "To do this, we'll break the text into chunks of seq_length+1. Suppose seq_length is 4 and our text is \"Hello\". Then, our input sequence is \"Hell\" and the target sequence is \"ello\".\n",
        "\n",
        "The batch method will then let us convert this stream of character indices to sequences of the desired size."
      ],
      "metadata": {
        "id": "xYD8Hf6fCqJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Batch definition to create training examples ###\n",
        "\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "\n",
        "      # the length of the vectorized songs string\n",
        "      n = vectorized_songs.shape[0] - 1\n",
        "\n",
        "      # randomly choose the starting indices for the examples in the training batch\n",
        "      idx = np.random.choice(n - seq_length, batch_size, replace = False)\n",
        "\n",
        "      # construct a list of input sequences for the training batch\n",
        "      input_batch = [vectorized_songs[i : i + seq_length] for i in idx]\n",
        "\n",
        "      # construct a list of input sequences for the training batch\n",
        "      output_batch = [vectorized_songs[i + 1 : i + seq_length + 1] for i in idx]\n",
        "\n",
        "      # Convert the input and output batches to tensors\n",
        "      x_batch = torch.tensor(input_batch, dtype=torch.long)\n",
        "      y_batch = torch.tensor(output_batch, dtype=torch.long)\n",
        "\n",
        "      return x_batch, y_batch\n",
        "\n",
        "# Perform some simple tests to make sure your batch function is working properly!\n",
        "test_args = (vectorized_songs, 10, 2)\n",
        "x_batch, y_batch = get_batch(*test_args)\n",
        "assert x_batch.shape == (2, 10), \"x_batch shape is incorrect\"\n",
        "assert y_batch.shape == (2, 10), \"y_batch shape is incorrect\"\n",
        "print(\"Batch function works correctly!\")"
      ],
      "metadata": {
        "id": "VY_bO9shCuGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of these vectors, each index is processed at a single time step. So, for the input at time step 0, the model receives the index for the first character in the sequence, and tries to predict the index of the next character. At the next timestep, it does the same thing, but the RNN considers the information from the previous step, i.e., its updated state, in addition to the current input.\n",
        "\n",
        "We can make this concrete by taking a look at how this works over the first several characters in our text:"
      ],
      "metadata": {
        "id": "7wwJTwSAN2TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
        "\n",
        "print(\"Time step | Input | Expected Output\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for i, (inp, tgt) in enumerate(zip(x_batch[0], y_batch[0])):\n",
        "    inp_char = idx2char[inp.item()]\n",
        "    tgt_char = idx2char[tgt.item()]\n",
        "    print(f\"{i:9} | {inp:5} ({inp_char}) | {tgt:15} ({tgt_char})\")\n"
      ],
      "metadata": {
        "id": "1qDqwQwyLvfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSug8EheOImv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  The Recurrent Neural Network (RNN) model\n",
        "Now we're ready to define and train an RNN model on our ABC music dataset, and then use that trained model to generate a new song. We'll train our RNN using batches of song snippets from our dataset, which we generated in the previous section.\n",
        "\n",
        "The model is based off the LSTM architecture, where we use a state vector to maintain information about the temporal relationships between consecutive characters. The final output of the LSTM is then fed into a fully connected linear nn.Linear layer where we'll output a softmax over each character in the vocabulary, and then sample from this distribution to predict the next character.\n",
        "\n",
        "As we introduced in the first portion of this lab, we'll be using PyTorch's nn.Module to define the model. Three components are used to define the model:\n",
        "\n",
        "nn.Embedding: This is the input layer, consisting of a trainable lookup table that maps the numbers of each character to a vector with embedding_dim dimensions.\n",
        "nn.LSTM: Our LSTM network, with size hidden_size.\n",
        "nn.Linear: The output layer, with vocab_size outputs.\n",
        "Drawing\n",
        "https://camo.githubusercontent.com/7d0de1b1f69f93e99d0d0deb39f201c51326016edc90f26066ec3e9c56186e1b/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4d4954446565704c6561726e696e672f696e74726f746f646565706c6561726e696e672f6d61737465722f6c6162312f696d672f6c73746d5f756e726f6c6c65642d30312d30312e706e67\n",
        "\n",
        "# Define the RNN model\n",
        "Let's define our model as an nn.Module."
      ],
      "metadata": {
        "id": "NbRlIgWHOVHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Layer 1: Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Layer 2: LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # Layer 3: Linear layer to project hidden state to vocabulary size\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        # Initialize hidden state and cell state with zeros\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(1, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "    def forward(self, x, state=None, return_state=False):\n",
        "        # Convert input indices to embeddings\n",
        "        x = self.embedding(x)  # [batch_size, seq_len] -> [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # Initialize hidden state if not provided\n",
        "        if state is None:\n",
        "            state = self.init_hidden(x.size(0), x.device)\n",
        "\n",
        "        # Forward pass through LSTM\n",
        "        out, state = self.lstm(x, state)  # out: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Map LSTM outputs to vocabulary space\n",
        "        out = self.fc(out)  # [batch_size, seq_len, vocab_size]\n",
        "\n",
        "        return out if not return_state else (out, state)\n"
      ],
      "metadata": {
        "id": "N-3-rwcNOZ5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The time has come! Let's instantiate the model!"
      ],
      "metadata": {
        "id": "abic-oQknTV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model! Build a simple model with default hyperparameters. You\n",
        "# will get the chance to change these later.\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "hidden_size = 1024\n",
        "batch_size = 8\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_size).to(device)\n",
        "\n",
        "# print out the summary of the model\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Mm6C7Z-HnT7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test out the RNN model\n",
        "It's always a good idea to run a few simple checks on our model to see that it behaves as expected.\n",
        "\n",
        "We can quickly check the layers in the model, the shape of the output of each of the layers, the batch size, and the dimensionality of the output. Note that the model can be run on inputs of any length."
      ],
      "metadata": {
        "id": "q4gVjs_Vq0VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model with sample data\n",
        "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
        "x = x.to(device)\n",
        "y = y.to(device)\n",
        "\n",
        "pred = model(x)\n",
        "\n",
        "print(\"Input shape:      \", x.shape, '# (batch_size, seq_length)')\n",
        "print(\"Prediction shape: \", pred.shape, '# (batch_size, seq_length, vocab_size)')"
      ],
      "metadata": {
        "id": "96GBdKbKnZNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions from the untrained model\n",
        "Let's take a look at what our untrained model is predicting.\n",
        "\n",
        "To get actual predictions from the model, we sample from the output distribution, which is defined by a torch.softmax over our character vocabulary. This will give us actual character indices. This means we are using a categorical distribution to sample over the example prediction. This gives a prediction of the next character (specifically its index) at each timestep. torch.multinomial samples over a categorical distribution to generate predictions.\n",
        "\n",
        "Note here that we sample from this probability distribution, as opposed to simply taking the argmax, which can cause the model to get stuck in a repetitive loop.\n",
        "\n",
        "Let's try this sampling out for the first example in the batch."
      ],
      "metadata": {
        "id": "0zVIpsUuq-4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = torch.multinomial(torch.softmax(pred[0], dim = -1), num_samples=1) # dim = -1 for vocab dim and num_samples = 1 step one\n",
        "sampled_indices = sampled_indices.squeeze().cpu().numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "id": "Lhx9ZxX3rAfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now decode these to see the text predicted by the untrained model:"
      ],
      "metadata": {
        "id": "y0EijdU6G5KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0].cpu()])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "metadata": {
        "id": "cJ6sqiCtrDxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the text predicted by the untrained model is pretty nonsensical! How can we do better? Well, we can train the network!"
      ],
      "metadata": {
        "id": "AtKGsy2SIMWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model: loss and training operations\n",
        "Now it's time to train the model!\n",
        "\n",
        "At this point, we can think of our next character prediction problem as a standard classification problem. Given the previous state of the RNN, as well as the input at a given time step, we want to predict the class of the next character -- that is, to actually predict the next character.\n",
        "\n",
        "To train our model on this classification task, we can use a form of the crossentropy loss (i.e., negative log likelihood loss). Specifically, we will use PyTorch's CrossEntropyLoss, as it combines the application of a log-softmax (LogSoftmax) and negative log-likelihood (NLLLoss in a single class and accepts integer targets for categorical classification tasks. We will want to compute the loss using the true targets -- the labels -- and the predicted targets -- the logits.\n",
        "\n",
        "Let's define a function to compute the loss, and then use that function to compute the loss using our example predictions from the untrained model."
      ],
      "metadata": {
        "id": "8w7baloyrfhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "def compute_loss(labels, logits):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      labels: (batch_size, sequence_length)\n",
        "      logits: (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "    Output:\n",
        "      loss: scalar cross entropy loss over the batch and sequence length\n",
        "    \"\"\"\n",
        "    # Flatten labels\n",
        "    batched_labels = labels.view(-1)\n",
        "\n",
        "    # Flatten logits\n",
        "    batched_logits = logits.view(-1, logits.size(-1))\n",
        "\n",
        "    # compute cross_entropy loss\n",
        "    loss = cross_entropy(batched_logits, batched_labels)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "hEPL581arhWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### compute the loss on the predictions from the untrained model from earlier. ###\n",
        "y.shape # (batch_size, seq_len)\n",
        "pred.shape # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "example_batch_loss = compute_loss(y, pred)\n",
        "\n",
        "print(f\"Prediction shape: {pred.shape} # (batch_size, sequence_length, vocab_size)\")\n",
        "print(f\"scalar_loss:      {example_batch_loss.mean().item()}\")"
      ],
      "metadata": {
        "id": "VC-jHYcGr_IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by defining some hyperparameters for training the model. To start, we have provided some reasonable values for some of the parameters. It is up to you to use what we've learned in class to help optimize the parameter selection here!"
      ],
      "metadata": {
        "id": "LQetDrG2sqat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "params = dict (\n",
        "    num_training_iteration = 3000, # increase this to train longer\n",
        "    batch_size = 8, # increase this the to train faster\n",
        "    seq_len = 100, # the len of each input sequence\n",
        "    learning_rate = 5e-2,\n",
        "    embedding_dim = 256,\n",
        "    hidden_size = 1024,\n",
        ")\n",
        "# Checkpoint location:\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "ql_DzSp2sH7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having defined our hyperparameters we can set up for experiment tracking with Comet. Experiment are the core objects in Comet and will allow us to track training and model development. Here we have written a short function to create a new Comet experiment. Note that in this setup, when hyperparameters change, you can run the create_experiment() function to initiate a new experiment. All experiments defined with the same project_name will live under that project in your Comet interface."
      ],
      "metadata": {
        "id": "JaOiTj_5tDGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a Comet experiment to track our training run ###\n",
        "\n",
        "def create_experiment():\n",
        "  # end any prior experiments\n",
        "  if 'experiment' in locals():\n",
        "    experiment.end()\n",
        "\n",
        "  # initiate the comet experiment for tracking\n",
        "  experiment = comet_ml.Experiment(\n",
        "                  api_key=COMET_API_KEY,\n",
        "                  project_name=\"6S191_Lab1_Part2\")\n",
        "  # log our hyperparameters, defined above, to the experiment\n",
        "  for param, value in params.items():\n",
        "    experiment.log_parameter(param, value)\n",
        "  experiment.flush()\n",
        "\n",
        "  return experiment"
      ],
      "metadata": {
        "id": "Ocdh5UvttDps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to define our training operation -- the optimizer and duration of training -- and use this function to train the model. You will experiment with the choice of optimizer and the duration for which you train your models, and see how these changes affect the network's output. Some optimizers you may like to try are Adam and Adagrad.\n",
        "\n",
        "First, we will instantiate a new model and an optimizer, and ready them for training. Then, we will use loss.backward(), enabled by PyTorch's autograd method, to perform the backpropagation. Finally, to update the model's parameters based on the computed gradients, we will utake a step with the optimizer, using optimizer.step().\n",
        "\n",
        "We will also generate a print-out of the model's progress through training, which will help us easily visualize whether or not we are minimizing the loss."
      ],
      "metadata": {
        "id": "KE9ZE5CVtQdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = LSTMModel(vocab_size, params['embedding_dim'], params['hidden_size'])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = params['learning_rate'])\n",
        "\n",
        "# loss function\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "# traning step\n",
        "def training_step(x, y):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  y_pred = model(x)\n",
        "\n",
        "  loss = compute_loss(y, y_pred)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss\n",
        "\n",
        "# Training loop\n",
        "history = []\n",
        "\n",
        "for iter in tqdm(range(params[\"num_training_iteration\"])):\n",
        "    x_batch, y_batch = get_batch(vectorized_songs, params[\"seq_len\"], params[\"batch_size\"])\n",
        "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
        "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
        "\n",
        "    loss = training_step(x_batch, y_batch)\n",
        "\n",
        "    history.append(loss.item())\n",
        "\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Step {iter}, Loss: {loss.item():.4f}\")\n",
        "        torch.save(model.state_dict(), checkpoint_prefix)\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), checkpoint_prefix)\n",
        "print(\"Training finished and model saved.\")"
      ],
      "metadata": {
        "id": "-aoQ-1CmtREa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Generate music using the RNN model\n",
        "Now, we can use our trained RNN model to generate some music! When generating music, we'll have to feed the model some sort of seed to get it started (because it can't predict anything without something to start with!).\n",
        "\n",
        "Once we have a generated seed, we can then iteratively predict each successive character (remember, we are using the ABC representation for our music) using our trained RNN. More specifically, recall that our RNN outputs a softmax over possible successive characters. For inference, we iteratively sample from these distributions, and then use our samples to encode a generated song in the ABC format.\n",
        "\n",
        "Then, all we have to do is write it to a file and listen!\n",
        "\n",
        "The prediction procedure\n",
        "Now, we're ready to write the code to generate text in the ABC music format:\n",
        "\n",
        "Initialize a \"seed\" start string and the RNN state, and set the number of characters we want to generate.\n",
        "\n",
        "Use the start string and the RNN state to obtain the probability distribution over the next predicted character.\n",
        "\n",
        "Sample from multinomial distribution to calculate the index of the predicted character. This predicted character is then used as the next input to the model.\n",
        "\n",
        "At each time step, the updated RNN state is fed back into the model, so that it now has more context in making the next prediction. After predicting the next character, the updated RNN states are again fed back into the model, which is how it learns sequence dependencies in the data, as it gets more information from the previous predictions.\n",
        "\n",
        "https://camo.githubusercontent.com/85f6271761cc48c50de28a63261c5922fb4c7f03cca29e899b2e2e3bbebf615a/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4d4954446565704c6561726e696e672f696e74726f746f646565706c6561726e696e672f6d61737465722f6c6162312f696d672f6c73746d5f696e666572656e63652e706e67\n",
        "\n",
        "Complete and experiment with this code block (as well as some of the aspects of network definition and training!), and see how the model performs. How do songs generated after training with a small number of epochs compare to those generated after a longer duration of training?"
      ],
      "metadata": {
        "id": "fTyzkpKctmwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, generation_length=1000):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    device = next(model.parameters()).device  # Get device (CPU/GPU)\n",
        "\n",
        "    # Convert the start string to indices\n",
        "    input_idx = [char2idx[c] for c in start_string]\n",
        "    input_idx = torch.tensor([input_idx], dtype=torch.long).to(device)  # shape: [1, len(start_string)]\n",
        "\n",
        "    # Initialize the LSTM hidden state and cell state\n",
        "    state = model.init_hidden(batch_size=1, device=device)\n",
        "\n",
        "    # Store generated characters\n",
        "    text_generated = []\n",
        "\n",
        "    tqdm._instances.clear()  # clear old tqdm instances\n",
        "\n",
        "    for _ in tqdm(range(generation_length)):\n",
        "        # Forward pass\n",
        "        predictions, state = model(input_idx, state, return_state=True)\n",
        "\n",
        "        # Take only the last character's predictions\n",
        "        predictions = predictions[:, -1, :]  # shape: [1, vocab_size]\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = torch.softmax(predictions, dim=-1)\n",
        "\n",
        "        # Sample the next character from the distribution\n",
        "        next_idx = torch.multinomial(probs, num_samples=1)  # shape: [1,1]\n",
        "\n",
        "        # Append predicted character\n",
        "        text_generated.append(idx2char[next_idx.item()])\n",
        "\n",
        "        # Prepare next input (keep batch dim = 1, seq_len = 1)\n",
        "        input_idx = next_idx  # shape: [1,1]\n",
        "\n",
        "    return start_string + \"\".join(text_generated)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "generated_song = generate_text(model, start_string=\"X:\", generation_length=500)\n",
        "print(\"Generated music:\\n\")\n",
        "print(generated_song)\n"
      ],
      "metadata": {
        "id": "kKqeH6_otzcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate ABC format text using the trained model\n",
        "# Start string \"X:\" because ABC files usually start with X: field\n",
        "generated_text = generate_text(model, start_string=\"X:\", generation_length=1000)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated ABC music text:\\n\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "RDHx3kL4t47K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play back the generated music!\n",
        "We can now call a function to convert the ABC format text to an audio file, and then play that back to check out our generated music! Try training longer if the resulting song is not long enough, or re-generating the song!\n",
        "\n",
        "We will save the song to Comet -- you will be able to find your songs under the Audio and Assets & Artifacts pages in your Comet interface for the project. Note the log_asset() documentation, where you will see how to specify file names and other parameters for saving your assets."
      ],
      "metadata": {
        "id": "9WIAObrtvcq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Play back generated songs ###\n",
        "\n",
        "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
        "\n",
        "for i, song in enumerate(generated_songs):\n",
        "  # Synthesize the waveform from a song\n",
        "  waveform = mdl.lab1.play_song(song)\n",
        "\n",
        "  # If its a valid song (correct syntax), lets play it!\n",
        "  if waveform:\n",
        "    print(\"Generated song\", i)\n",
        "    ipythondisplay.display(waveform)\n",
        "\n",
        "    numeric_data = np.frombuffer(waveform.data, dtype=np.int16)\n",
        "    wav_file_path = f\"output_{i}.wav\"\n",
        "    write(wav_file_path, 88200, numeric_data)\n",
        "\n",
        "    # save your song to the Comet interface -- you can access it there\n",
        "    experiment = create_experiment()\n",
        "    experiment.log_asset(wav_file_path)"
      ],
      "metadata": {
        "id": "USvDz4KsvUYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when done, end the comet experiment\n",
        "experiment.end()"
      ],
      "metadata": {
        "id": "QOzVl6-Vvja1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}